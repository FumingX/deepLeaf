{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas\n",
    "from scipy.misc import imread, imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import random\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "DATA_PATH = '/home/zhaoyu/deepLeaf/Data/'\n",
    "PROJECT_PATH = '/home/zhaoyu/deepLeaf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_RESIZE_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the train and test data\n",
    "\n",
    "train = pandas.read_csv(DATA_PATH + '/train.csv')\n",
    "test = pandas.read_csv(DATA_PATH + '/test.csv')\n",
    "\n",
    "# get the pre-extracted features\n",
    "feat_train = train.copy()\n",
    "feat_test = test.copy()\n",
    "feat_train = feat_train.drop(['id', 'species'], axis=1)\n",
    "feat_test = feat_test.drop(['id'], axis=1)\n",
    "feat_train = StandardScaler().fit(feat_train).transform(feat_train)\n",
    "feat_test = StandardScaler().fit(feat_test).transform(feat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Method 1: Resize directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "# image_data contains ALL images from the images folder\n",
    "\n",
    "# method 1: resize directly\n",
    "image_data = {}\n",
    "for img_file in os.listdir(DATA_PATH + '/images'):\n",
    "    resized_img = imresize(imread(DATA_PATH + '/images/' + img_file), (IMAGE_RESIZE_SIZE,IMAGE_RESIZE_SIZE)).astype(np.float32)\n",
    "    image_data[img_file.split(\".\")[0]] = resized_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Method 2: Padding first, then resize (image will not be disturbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method 2: pad first, then resize\n",
    "image_data = {}\n",
    "for img_file in os.listdir(DATA_PATH + '/images'):\n",
    "    img = imread(DATA_PATH + '/images/' + img_file)\n",
    "    h, w = img.shape\n",
    "    max_dim = max(h, w)\n",
    "    padded_img = np.lib.pad(img, \n",
    "                     (((max_dim-h)//2, max_dim-h-(max_dim-h)//2), ((max_dim-w)//2, max_dim-w-(max_dim-w)//2)), \n",
    "                     'constant', constant_values=1)\n",
    "    resized_img = imresize(padded_img, (IMAGE_RESIZE_SIZE, IMAGE_RESIZE_SIZE)).astype(np.float32)\n",
    "    image_data[img_file.split(\".\")[0]] = resized_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Method 3: rotation/flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# codes is from https://www.kaggle.com/abhmul/leaf-classification/keras-convnet-lb-0-0052-w-visualization\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator, array_to_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 194)\n",
      "(990, 192)\n",
      "(594, 193)\n",
      "(594, 192)\n",
      "1584\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(feat_train.shape)\n",
    "print(test.shape)\n",
    "print(feat_test.shape)\n",
    "print(len(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for key, value in image_data.items() :\n",
    "#     print (key, value)\n",
    "#     exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_data['2'], cmap='gray')\n",
    "print(image_data['1'].shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get labels\n",
    "le = LabelEncoder()\n",
    "le.fit(train.species)\n",
    "# print(le.classes_)\n",
    "# print(len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = le.transform(train.species)\n",
    "# print(train_labels)\n",
    "# print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 64, 64, 1)\n",
      "(594, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# separate train and test from image_data\n",
    "image_train = np.array([image_data[str(idx)] for idx in train.id])\n",
    "image_test = np.array([image_data[str(idx)] for idx in test.id])\n",
    "image_train = np.expand_dims(image_train, axis=4)\n",
    "image_test = np.expand_dims(image_test, axis=4)\n",
    "print(image_train.shape)\n",
    "print(image_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train_splitted shape (891, 64, 64, 1)\n",
      "image_validation_splitted shape (99, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# split train into train and validation\n",
    "# Use heavy data augmentation\n",
    "# image_train (990, 64, 64, 1)\n",
    "# train_labels_cat (990, 99)\n",
    "\n",
    "train_numbers = int(len(image_train) * 0.9)\n",
    "val_numbers = len(image_train) - train_numbers\n",
    "train_indices = random.sample(range(0, len(image_train)), train_numbers)\n",
    "val_indices = [x for x in range(0, len(image_train)) if x not in train_indices]\n",
    "\n",
    "image_train_splitted = image_train[train_indices, :, :, :]\n",
    "image_train_label_splitted = train_labels_cat[train_indices, :]\n",
    "image_validation_splitted = image_train[val_indices, :, :, :]\n",
    "image_validation_label_splitted = train_labels_cat[val_indices, :]\n",
    "\n",
    "print('image_train_splitted shape', image_train_splitted.shape)\n",
    "print('image_validation_splitted shape', image_validation_splitted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image generator for splitted train and validation\n",
    "imgen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "imgen.fit(image_train_splitted)\n",
    "imgen_flow = imgen.flow(image_train_splitted, image_train_label_splitted, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # randomly split original train into train and validation\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "# for train_index, val_index in sss.split(image_train, train_labels):\n",
    "#     image_train_X, image_val_X = image_train[train_index], image_train[val_index]\n",
    "#     image_train_Y, image_val_Y = train_labels[train_index], train_labels[val_index]\n",
    "#     print(image_train_X.shape)\n",
    "#     print(image_train_Y.shape)\n",
    "#     print(image_val_X.shape)\n",
    "#     print(image_val_Y.shape)\n",
    "#     print(\"TRAIN:\", train_index)\n",
    "#     print(\"VAL:\", val_index)\n",
    "#     plt.imshow(image_train_X[0, :, :, 0], cmap='gray')\n",
    "#     plt.show()\n",
    "#     print(train_index[0])\n",
    "#     print(image_train_Y[0])\n",
    "#     print(le.inverse_transform([image_train_Y[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 64, 64, 1)\n",
      "(990, 99)\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Convert class vectors to binary class matrices (one-hot encoding)\n",
    "num_classes = 99\n",
    "train_labels_cat = keras.utils.to_categorical(train_labels, num_classes)\n",
    "print(image_train.shape)\n",
    "print(train_labels_cat.shape)\n",
    "print(train_labels_cat[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build keras model for Images only\n",
    "### model 1: following the CIFAR 10 example from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_model = Sequential()\n",
    "\n",
    "img_model.add(Conv2D(32, (3, 3), padding='same', input_shape=image_train.shape[1:]))\n",
    "img_model.add(Activation('relu'))\n",
    "img_model.add(Conv2D(32, (3, 3)))\n",
    "img_model.add(Activation('relu'))\n",
    "img_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "img_model.add(Dropout(0.25))\n",
    "\n",
    "img_model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "img_model.add(Activation('relu'))\n",
    "img_model.add(Conv2D(64, (3, 3)))\n",
    "img_model.add(Activation('relu'))\n",
    "img_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "img_model.add(Dropout(0.25))\n",
    "\n",
    "img_model.add(Flatten())\n",
    "img_model.add(Dense(512))\n",
    "img_model.add(Activation('relu'))\n",
    "img_model.add(Dropout(0.5))\n",
    "img_model.add(Dense(num_classes))\n",
    "img_model.add(Activation('softmax'))\n",
    "\n",
    "img_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 62, 62, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 29, 29, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 29, 29, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               6423040   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 99)                50787     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 99)                0         \n",
      "=================================================================\n",
      "Total params: 6,538,819\n",
      "Trainable params: 6,538,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# output the model summary\n",
    "img_model.count_params()\n",
    "img_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "62/61 [==============================] - 0s - loss: 4.6095 - acc: 0.0060 - val_loss: 5.0361 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "62/61 [==============================] - 0s - loss: 4.5912 - acc: 0.0131 - val_loss: 15.3930 - val_acc: 0.0202\n",
      "Epoch 3/20\n",
      "62/61 [==============================] - 0s - loss: 4.4424 - acc: 0.0267 - val_loss: 15.1321 - val_acc: 0.0404\n",
      "Epoch 4/20\n",
      "62/61 [==============================] - 0s - loss: 4.1812 - acc: 0.0478 - val_loss: 14.7992 - val_acc: 0.0808\n",
      "Epoch 5/20\n",
      "62/61 [==============================] - 0s - loss: 3.9860 - acc: 0.0650 - val_loss: 14.4929 - val_acc: 0.1010\n",
      "Epoch 6/20\n",
      "62/61 [==============================] - 0s - loss: 3.8442 - acc: 0.0917 - val_loss: 13.9912 - val_acc: 0.1111\n",
      "Epoch 7/20\n",
      "62/61 [==============================] - 0s - loss: 3.7296 - acc: 0.0962 - val_loss: 13.0388 - val_acc: 0.1818\n",
      "Epoch 8/20\n",
      "62/61 [==============================] - 0s - loss: 3.5765 - acc: 0.1214 - val_loss: 13.8455 - val_acc: 0.1313\n",
      "Epoch 9/20\n",
      "62/61 [==============================] - 0s - loss: 3.4116 - acc: 0.1728 - val_loss: 14.3365 - val_acc: 0.1010\n",
      "Epoch 10/20\n",
      "62/61 [==============================] - 0s - loss: 3.2937 - acc: 0.1824 - val_loss: 15.2636 - val_acc: 0.0505\n",
      "Epoch 11/20\n",
      "62/61 [==============================] - 0s - loss: 3.2072 - acc: 0.1749 - val_loss: 13.8388 - val_acc: 0.1414\n",
      "Epoch 12/20\n",
      "62/61 [==============================] - 0s - loss: 3.1435 - acc: 0.2081 - val_loss: 13.0089 - val_acc: 0.1818\n",
      "Epoch 13/20\n",
      "62/61 [==============================] - 0s - loss: 2.9745 - acc: 0.2302 - val_loss: 12.5957 - val_acc: 0.2020\n",
      "Epoch 14/20\n",
      "62/61 [==============================] - 0s - loss: 3.0390 - acc: 0.2091 - val_loss: 13.8101 - val_acc: 0.1414\n",
      "Epoch 15/20\n",
      "62/61 [==============================] - 0s - loss: 2.8735 - acc: 0.2634 - val_loss: 13.5132 - val_acc: 0.1616\n",
      "Epoch 16/20\n",
      "62/61 [==============================] - 0s - loss: 2.8153 - acc: 0.2648 - val_loss: 14.3287 - val_acc: 0.1111\n",
      "Epoch 17/20\n",
      "62/61 [==============================] - 0s - loss: 2.7590 - acc: 0.2656 - val_loss: 12.5028 - val_acc: 0.2121\n",
      "Epoch 18/20\n",
      "62/61 [==============================] - 0s - loss: 2.7156 - acc: 0.2658 - val_loss: 13.8681 - val_acc: 0.1313\n",
      "Epoch 19/20\n",
      "62/61 [==============================] - 0s - loss: 2.7220 - acc: 0.2686 - val_loss: 13.1977 - val_acc: 0.1717\n",
      "Epoch 20/20\n",
      "62/61 [==============================] - 0s - loss: 2.5835 - acc: 0.2942 - val_loss: 13.3250 - val_acc: 0.1717\n"
     ]
    }
   ],
   "source": [
    "# img_history = img_model.fit(image_train_splitted, image_train_label_splitted,\n",
    "#                             batch_size=16,\n",
    "#                             epochs=20,\n",
    "#                             #validation_split=0.1,\n",
    "#                             validation_data=(image_validation_splitted, image_validation_label_splitted),\n",
    "#                             shuffle=True)\n",
    "\n",
    "img_history = img_model.fit_generator(imgen_flow,\n",
    "                                      epochs=20,\n",
    "                                      validation_data=(image_validation_splitted, image_validation_label_splitted),\n",
    "                                      steps_per_epoch=len(image_train)/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(img_history.history['val_acc'])\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: A simpler CNN model using data augmentation method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_model2 = Sequential()\n",
    "\n",
    "img_model2.add(Conv2D(16, (3, 3), padding='same', input_shape=image_train.shape[1:]))\n",
    "img_model2.add(Activation('relu'))\n",
    "img_model2.add(Conv2D(16, (3, 3)))\n",
    "img_model2.add(Activation('relu'))\n",
    "img_model2.add(Conv2D(16, (3, 3)))\n",
    "img_model2.add(Activation('relu'))\n",
    "\n",
    "img_model2.add(Flatten())\n",
    "img_model2.add(Dense(128))\n",
    "img_model2.add(Activation('relu'))\n",
    "img_model2.add(Dense(64))\n",
    "img_model2.add(Activation('relu'))\n",
    "img_model2.add(Dense(num_classes))\n",
    "img_model2.add(Activation('softmax'))\n",
    "\n",
    "img_model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the model summary\n",
    "# img_model.count_params()\n",
    "# img_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train_splitted shape (891, 64, 64, 1)\n",
      "image_validation_splitted shape (99, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# img_history = img_model2.fit(image_train, train_labels_cat,\n",
    "#                              batch_size=16,\n",
    "#                              epochs=10,\n",
    "#                              validation_split=0.1,\n",
    "#                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "62/61 [==============================] - 1s - loss: 4.6236 - acc: 0.0101 - val_loss: 16.0496 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "62/61 [==============================] - 1s - loss: 4.5413 - acc: 0.0171 - val_loss: 15.3355 - val_acc: 0.0101\n",
      "Epoch 3/20\n",
      "62/61 [==============================] - 1s - loss: 4.2913 - acc: 0.0363 - val_loss: 15.3836 - val_acc: 0.0303\n",
      "Epoch 4/20\n",
      "62/61 [==============================] - 1s - loss: 4.1016 - acc: 0.0509 - val_loss: 14.9654 - val_acc: 0.0606\n",
      "Epoch 5/20\n",
      "62/61 [==============================] - 1s - loss: 3.8571 - acc: 0.0897 - val_loss: 14.2380 - val_acc: 0.1010\n",
      "Epoch 6/20\n",
      "62/61 [==============================] - 1s - loss: 3.6863 - acc: 0.1124 - val_loss: 13.8377 - val_acc: 0.1313\n",
      "Epoch 7/20\n",
      "62/61 [==============================] - 1s - loss: 3.5419 - acc: 0.1426 - val_loss: 12.9919 - val_acc: 0.1717\n",
      "Epoch 8/20\n",
      "62/61 [==============================] - 1s - loss: 3.4155 - acc: 0.1577 - val_loss: 13.8618 - val_acc: 0.1111\n",
      "Epoch 9/20\n",
      "62/61 [==============================] - 1s - loss: 3.3433 - acc: 0.1718 - val_loss: 12.6720 - val_acc: 0.2020\n",
      "Epoch 10/20\n",
      "62/61 [==============================] - 1s - loss: 3.2302 - acc: 0.1873 - val_loss: 12.7394 - val_acc: 0.2020\n",
      "Epoch 11/20\n",
      "62/61 [==============================] - 1s - loss: 3.0749 - acc: 0.2181 - val_loss: 13.0247 - val_acc: 0.1919\n",
      "Epoch 12/20\n",
      "62/61 [==============================] - 1s - loss: 3.0528 - acc: 0.2025 - val_loss: 12.4850 - val_acc: 0.2121\n",
      "Epoch 13/20\n",
      "62/61 [==============================] - 1s - loss: 2.9493 - acc: 0.2272 - val_loss: 11.9898 - val_acc: 0.2222\n",
      "Epoch 14/20\n",
      "62/61 [==============================] - 1s - loss: 2.9113 - acc: 0.2337 - val_loss: 12.0479 - val_acc: 0.2525\n",
      "Epoch 15/20\n",
      "62/61 [==============================] - 1s - loss: 2.7784 - acc: 0.2554 - val_loss: 11.9161 - val_acc: 0.2525\n",
      "Epoch 16/20\n",
      "62/61 [==============================] - 0s - loss: 2.7254 - acc: 0.2731 - val_loss: 11.9022 - val_acc: 0.2424\n",
      "Epoch 17/20\n",
      "62/61 [==============================] - 1s - loss: 2.7006 - acc: 0.2581 - val_loss: 11.0080 - val_acc: 0.3030\n",
      "Epoch 18/20\n",
      "62/61 [==============================] - 1s - loss: 2.5925 - acc: 0.2836 - val_loss: 12.3765 - val_acc: 0.2121\n",
      "Epoch 19/20\n",
      "62/61 [==============================] - 1s - loss: 2.5310 - acc: 0.2906 - val_loss: 12.8239 - val_acc: 0.2020\n",
      "Epoch 20/20\n",
      "62/61 [==============================] - 1s - loss: 2.4402 - acc: 0.3125 - val_loss: 10.9771 - val_acc: 0.3030\n"
     ]
    }
   ],
   "source": [
    "img_history = img_model2.fit_generator(imgen_flow,\n",
    "                                       epochs=20,\n",
    "                                       validation_data=(image_validation_splitted, image_validation_label_splitted),\n",
    "                                       steps_per_epoch=len(image_train)/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(img_history.history['val_acc'])\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build keras model for Features only\n",
    "feat_model = Sequential()\n",
    "feat_model.add(Dense(512, input_dim=192, kernel_initializer='uniform', activation='relu'))\n",
    "feat_model.add(Dropout(0.3))\n",
    "feat_model.add(Dense(256, activation='sigmoid'))\n",
    "feat_model.add(Dropout(0.3))\n",
    "feat_model.add(Dense(99, activation='softmax'))\n",
    "\n",
    "feat_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the model summary\n",
    "feat_model.count_params()\n",
    "feat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_history = feat_model.fit(feat_train, train_labels_cat,\n",
    "                              batch_size=16,\n",
    "                              epochs=50,\n",
    "                              validation_split=0.1,\n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(feat_history.history['val_acc'])\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do prediction\n",
    "pred = feat_model.predict_proba(feat_test)\n",
    "columns = sorted(train.species.unique())\n",
    "pred = pandas.DataFrame(pred, index=test.id, columns=columns)\n",
    "output = open('prediction_fully_connected.csv','w')\n",
    "output.write(pred.to_csv())\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
